# ðŸ§ª Testing Guide - Assistant Brain OS

## ðŸ“‹ Overview

Comprehensive test suite to ensure system robustness and reliability.

---

## ðŸŽ¯ Test Coverage

### 1. **Agent Tests** (`test_agents.py`)

Tests all three AI agents:

**Archivist Tests:**
- âœ… Successful knowledge saving
- âœ… Successful knowledge searching
- âœ… Error handling
- âœ… Empty text handling

**Researcher Tests:**
- âœ… Successful research execution
- âœ… Topic-based research
- âœ… Error handling

**Writer Tests:**
- âœ… Successful content writing
- âœ… Writing with research data
- âœ… Different format types
- âœ… Error handling

### 2. **Message Handling Tests** (`test_message_handling.py`)

Tests message routing and detection:

**Casual Message Detection:**
- âœ… Greetings detection
- âœ… Thanks detection
- âœ… Goodbye detection
- âœ… "How are you" detection
- âœ… Actionable messages not detected as casual
- âœ… Short unclear messages
- âœ… Messages with keywords

**Intent Routing:**
- âœ… Save/archive intent
- âœ… Research intent
- âœ… Writing intent
- âœ… Default fallback

**User Settings:**
- âœ… Default settings retrieval
- âœ… Existing settings retrieval
- âœ… Settings persistence

**Thinking Messages:**
- âœ… Message existence
- âœ… Uniqueness
- âœ… Non-empty validation

### 3. **Contract Tests** (`test_contracts.py`)

Tests data models and validation:

**Job Model:**
- âœ… Creation with defaults
- âœ… Creation with custom values
- âœ… Status enumeration
- âœ… Validation
- âœ… History management
- âœ… Serialization

**Knowledge Entry:**
- âœ… Entry creation
- âœ… Metadata handling
- âœ… Validation
- âœ… Empty tags

**Agent Response:**
- âœ… Success responses
- âœ… Failure responses
- âœ… Agent chaining
- âœ… Additional data
- âœ… Validation

### 4. **Worker Tests** (`test_worker.py`)

Tests job processing:

**Job Processing:**
- âœ… Successful processing
- âœ… Failed job retry
- âœ… Max retries exceeded
- âœ… Agent chaining

**Dynamic Agent Loading:**
- âœ… Load archivist
- âœ… Load researcher
- âœ… Load writer

### 5. **Integration Tests** (`test_integration.py`)

Tests end-to-end workflows:

**Workflows:**
- âœ… Save and search workflow
- âœ… Research and write workflow

**Error Recovery:**
- âœ… Agent failure recovery
- âœ… Malformed payload handling

**Concurrency:**
- âœ… Multiple agents in parallel

**Data Persistence:**
- âœ… Job serialization/deserialization
- âœ… Response serialization

### 6. **Configuration Tests** (`test_config.py`)

Tests configuration loading:

**Configuration:**
- âœ… Required vars loading
- âœ… Default values
- âœ… Database paths

---

## ðŸš€ Running Tests

### Quick Start

```bash
# Run all tests
./run_tests.sh
```

### Manual Commands

```bash
# Activate virtual environment
source venv/bin/activate

# Run all tests
python -m pytest tests/ -v

# Run specific test file
python -m pytest tests/test_agents.py -v

# Run specific test class
python -m pytest tests/test_agents.py::TestArchivist -v

# Run specific test
python -m pytest tests/test_agents.py::TestArchivist::test_archivist_save_success -v

# Run with coverage
python -m pytest tests/ --cov=. --cov-report=html

# Run only unit tests (exclude integration)
python -m pytest tests/ -m "not integration"

# Run only integration tests
python -m pytest tests/test_integration.py -v
```

---

## ðŸ“Š Test Output

### Successful Run

```
ðŸ§ª Running Assistant Brain OS Test Suite
========================================

ðŸ“ Running unit tests...
tests/test_agents.py::TestArchivist::test_archivist_save_success PASSED
tests/test_agents.py::TestArchivist::test_archivist_search_success PASSED
tests/test_agents.py::TestArchivist::test_archivist_error_handling PASSED
...

======== 45 passed in 5.23s ========

âœ… Test suite complete!
```

### Failed Test

```
tests/test_agents.py::TestArchivist::test_archivist_save_success FAILED

FAILED tests/test_agents.py::TestArchivist::test_archivist_save_success
AssertionError: assert False is True
```

---

## ðŸ› ï¸ Writing New Tests

### Test Structure

```python
import pytest
from unittest.mock import Mock, patch, AsyncMock

class TestYourFeature:
    """Test your feature"""

    @pytest.mark.asyncio
    async def test_feature_success(self):
        """Test successful feature execution"""
        # Arrange
        input_data = {"key": "value"}

        # Act
        result = await your_function(input_data)

        # Assert
        assert result.success is True
        assert result.output != ""
```

### Mocking Guidelines

**Mock external APIs:**
```python
with patch('agents.archivist.archivist_agent.run') as mock_run:
    mock_run.return_value = AsyncMock(output="Success")
    result = await archivist.execute(payload)
```

**Mock Redis:**
```python
with patch('worker.r') as mock_redis:
    mock_redis.lpush.return_value = True
    # Test code
```

**Mock Telegram bot:**
```python
with patch('worker.bot.send_message', new_callable=AsyncMock) as mock_send:
    await process_job(job_data)
    mock_send.assert_called_once()
```

### Test Best Practices

1. **Use descriptive names**
   - âœ… `test_archivist_save_success`
   - âŒ `test_1`

2. **Test one thing per test**
   - Each test should verify one specific behavior

3. **Use AAA pattern**
   - **Arrange:** Set up test data
   - **Act:** Execute the function
   - **Assert:** Verify results

4. **Mock external dependencies**
   - Don't make real API calls
   - Don't access real databases
   - Don't send real Telegram messages

5. **Test edge cases**
   - Empty inputs
   - Invalid inputs
   - Network errors
   - Timeouts

---

## ðŸŽ¯ Test Markers

Use markers to categorize tests:

```python
@pytest.mark.asyncio  # Async test
@pytest.mark.slow     # Slow running test
@pytest.mark.integration  # Integration test
@pytest.mark.unit     # Unit test
```

Run specific markers:
```bash
# Run only slow tests
pytest -m slow

# Run all except slow tests
pytest -m "not slow"

# Run unit tests only
pytest -m unit
```

---

## ðŸ“ˆ Coverage Report

After running tests with coverage:

```bash
# View in terminal
pytest tests/ --cov=. --cov-report=term-missing

# Generate HTML report
pytest tests/ --cov=. --cov-report=html

# Open HTML report
open htmlcov/index.html  # macOS
xdg-open htmlcov/index.html  # Linux
```

**Coverage targets:**
- âœ… Aim for >80% overall coverage
- âœ… 100% coverage for critical paths
- âœ… All agents fully tested
- âœ… Error handling tested

---

## ðŸ› Debugging Failed Tests

### Verbose Output

```bash
# Show detailed output
pytest tests/ -vv

# Show print statements
pytest tests/ -s

# Show full traceback
pytest tests/ --tb=long
```

### Run Failed Tests Only

```bash
# Run tests that failed last time
pytest --lf

# Run failed tests first, then others
pytest --ff
```

### Debug Mode

```python
# Add breakpoint in test
def test_something():
    import pdb; pdb.set_trace()
    # Test code
```

Run with:
```bash
pytest tests/ -s  # -s allows pdb interaction
```

---

## âœ… Continuous Integration

### Pre-commit Hook

Create `.git/hooks/pre-commit`:

```bash
#!/bin/bash
echo "Running tests before commit..."
./run_tests.sh
if [ $? -ne 0 ]; then
    echo "Tests failed! Commit aborted."
    exit 1
fi
```

Make executable:
```bash
chmod +x .git/hooks/pre-commit
```

### GitHub Actions (Example)

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.12
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov
      - name: Run tests
        run: ./run_tests.sh
```

---

## ðŸ“š Additional Test Files Needed

### Future Test Additions

1. **`test_database.py`** - Database operations
   - ChromaDB interactions
   - SQLite operations
   - Vector search

2. **`test_telegram.py`** - Telegram bot handlers
   - Command handlers
   - Message handlers
   - Callback queries

3. **`test_redis.py`** - Redis queue operations
   - Queue push/pop
   - Job serialization
   - Queue management

4. **`test_performance.py`** - Performance tests
   - Response time
   - Concurrent users
   - Memory usage

---

## ðŸŽ“ Test Maintenance

### Regular Tasks

- âœ… Run tests before commits
- âœ… Update tests when code changes
- âœ… Add tests for new features
- âœ… Fix broken tests immediately
- âœ… Review coverage reports weekly
- âœ… Remove obsolete tests

### Test Health Metrics

Monitor:
- Test count (should grow with features)
- Coverage percentage (aim for >80%)
- Test execution time (keep fast)
- Flaky tests (fix immediately)
- Test failures (investigate all)

---

## ðŸ“ Test Checklist

Before deployment:

- [ ] All tests passing
- [ ] Coverage >80%
- [ ] Integration tests passing
- [ ] Error handling tested
- [ ] Edge cases covered
- [ ] Performance acceptable
- [ ] No flaky tests
- [ ] Documentation updated

---

## ðŸš€ Quick Commands Reference

```bash
# Run all tests
./run_tests.sh

# Run specific file
pytest tests/test_agents.py -v

# Run with coverage
pytest tests/ --cov=. --cov-report=html

# Run only fast tests
pytest tests/ -m "not slow"

# Debug mode
pytest tests/ -s --pdb

# Watch mode (rerun on changes)
pytest-watch tests/
```

---

**ðŸŽ‰ Happy Testing! Ensure your brain is robust!**
