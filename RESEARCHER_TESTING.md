# ðŸ§ª Deep Researcher Testing Guide

## Overview

Comprehensive test suite for the multi-source deep researcher agent. These tests use **REAL API calls** (not mocked) to verify actual functionality, edge cases, and logging.

## Test Categories

### 1. **Basic Execution Tests** (`TestResearcherBasicExecution`)
- âœ… Simple research queries
- âœ… Multi-source aggregation (Tavily + DuckDuckGo + Brave)
- âœ… Caching behavior

### 2. **Edge Case Tests** (`TestResearcherEdgeCases`)
- âœ… Empty queries
- âœ… Very long queries (>1000 chars)
- âœ… Special characters (@#$%^&*)
- âœ… Obscure/nonexistent topics
- âœ… Multiple questions in one query

### 3. **Query Analysis Tests** (`TestResearcherQueryAnalysis`)
- âœ… Complex query breakdown into sub-questions
- âœ… analyze_query() tool execution

### 4. **API Integration Tests** (`TestResearcherAPIIntegration`)
- âœ… Tavily search execution
- âœ… DuckDuckGo search execution
- âœ… Brave search execution (if configured)

### 5. **Error Handling Tests** (`TestResearcherErrorHandling`)
- âœ… API failure recovery
- âœ… Timeout handling
- âœ… Malformed response handling

### 6. **Logging Tests** (`TestResearcherLogging`)
- âœ… Verify logging during research
- âœ… Verify error logging

### 7. **Performance Tests** (`TestResearcherPerformance`)
- âœ… Concurrent research requests
- âœ… Cache effectiveness
- âœ… Response time benchmarks

### 8. **Output Quality Tests** (`TestResearcherOutputQuality`)
- âœ… Output format validation
- âœ… Source citation verification
- âœ… Hallucination detection

## Running the Tests

### Quick Run (All Tests)
```bash
cd /root/assistant-brain-os
./run_researcher_tests.sh
```

### Run Specific Test Class
```bash
source venv/bin/activate
pytest tests/test_researcher_deep.py::TestResearcherBasicExecution -v
```

### Run Single Test
```bash
source venv/bin/activate
pytest tests/test_researcher_deep.py::TestResearcherBasicExecution::test_simple_research_query -v -s
```

### Run with Live Logging
```bash
source venv/bin/activate
pytest tests/test_researcher_deep.py -v --log-cli-level=INFO --capture=no -s
```

### Run Only Edge Case Tests
```bash
source venv/bin/activate
pytest tests/test_researcher_deep.py::TestResearcherEdgeCases -v
```

## What to Expect

### Output Example
```
================================================
   DEEP RESEARCHER TEST SUITE
================================================

tests/test_researcher_deep.py::TestResearcherBasicExecution::test_simple_research_query
INFO     âœ… Simple research completed in 8.32s
INFO     Output length: 487 chars
PASSED

tests/test_researcher_deep.py::TestResearcherEdgeCases::test_empty_query
INFO     âœ… Empty query handled
PASSED

tests/test_researcher_deep.py::TestResearcherAPIIntegration::test_tavily_search_execution
INFO     âœ… Tavily search executed successfully
INFO     Result length: 1243 chars
PASSED

================================================
RESEARCHER PERFORMANCE SUMMARY
================================================
âœ… What is Python?                              |  7.23s |  487 chars
âœ… Explain machine learning basics              | 12.45s |  823 chars
âœ… Research quantum computing                   | 28.67s | 1456 chars
================================================
Average Duration: 16.12s
Success Rate: 100%
================================================
```

## Test Execution Times

| Test Category | Expected Time | Notes |
|--------------|---------------|-------|
| Basic Execution | 30-60s | 3 real API calls |
| Edge Cases | 60-120s | 6 tests, some may timeout |
| API Integration | 20-40s | 3 API tests |
| Error Handling | 30-60s | Includes failure scenarios |
| Performance | 60-90s | Concurrent execution tests |
| Output Quality | 30-60s | Quality validation |
| **Total Suite** | **5-10 minutes** | ~30 tests with real APIs |

## Important Notes

### âš ï¸ Real API Calls
These tests make **actual API calls** to:
- Tavily (costs $0.002 per search)
- DuckDuckGo (free)
- Brave (free tier)

**Cost estimate**: Running full suite ~50-100 times = 60-120 searches â‰ˆ $0.12-0.24

### âš ï¸ API Rate Limits
- Tavily: 1000 searches/month free tier
- DuckDuckGo: May throttle if too many requests
- Brave: 2000 searches/month free tier

**Recommendation**: Run full suite sparingly, focus on specific test classes when debugging.

### âœ… What Gets Tested (NO MOCKS)

1. **Actual Tavily API calls** - Real search results
2. **Actual DuckDuckGo searches** - Real web results
3. **Real ChromaDB caching** - Actual cache reads/writes
4. **Real DeepSeek API calls** - Actual query analysis
5. **Real tool execution** - browse_page() uses Playwright
6. **Real error scenarios** - Actual API failures

## Edge Cases Covered

### 1. Empty Query
```python
payload = {"text": "", "user_id": 12345}
```
**Expected**: Fails gracefully or returns minimal response

### 2. Very Long Query
```python
long_query = "Research " + " ".join([f"topic{i}" for i in range(200)])
```
**Expected**: Handles without crashing

### 3. Special Characters
```python
payload = {"text": "What is C++ & C#? @#$%"}
```
**Expected**: Processes correctly, returns relevant results

### 4. Obscure Topic
```python
payload = {"text": "Research xyzabc123nonexistent"}
```
**Expected**: Acknowledges lack of results gracefully

### 5. Multiple Questions
```python
payload = {"text": "What is AI? How does it work? What are its applications?"}
```
**Expected**: Addresses all questions comprehensively

### 6. Concurrent Requests
```python
# 3 simultaneous research tasks
```
**Expected**: All complete successfully, faster than sequential

## Logging Verification

Tests verify that proper logging occurs:

```python
# Expected log messages:
"ðŸ”„ Processing Job: abc123 | Agent: researcher"
"Tavily search: quantum computing"
"DuckDuckGo search: quantum computing 2026"
"âœ… Job abc123 completed | Duration: 23s"
```

## Quality Checks

### Output Must:
1. âœ… Be >200 characters (comprehensive)
2. âœ… Be <5000 characters (not excessive)
3. âœ… Contain relevant keywords
4. âœ… Include source citations
5. âœ… NOT contain hallucination phrases ("I can help", "I'd be happy to")

### Timing Must:
1. âœ… Simple queries: <15 seconds
2. âœ… Complex queries: <45 seconds
3. âœ… Deep research: <90 seconds

### API Integration Must:
1. âœ… Tavily returns AI summaries + sources
2. âœ… DuckDuckGo returns web results + URLs
3. âœ… Cache reduces repeat query time

## Troubleshooting Tests

### Test Fails: "Tavily API not available"
**Cause**: TAVILY_API_KEY not set or invalid
**Fix**:
```bash
# Check .env file
cat .env | grep TAVILY
# Should show: TAVILY_API_KEY=tvly-dev-...

# Restart workers to load env
pm2 restart all --update-env
```

### Test Fails: "DuckDuckGo search failed"
**Cause**: Network issue or rate limiting
**Fix**: Wait a few minutes, try again

### Test Fails: "AssertionError: Should return substantial content"
**Cause**: API returned minimal results
**Check**:
```bash
# Run test with verbose output
pytest tests/test_researcher_deep.py::TestResearcherBasicExecution::test_simple_research_query -v -s

# Check actual output length in logs
```

### Tests Take Too Long
**Cause**: Real API calls are slow
**Expected**: 5-10 minutes for full suite
**Solution**: Run specific test classes instead of full suite

## Running Tests in CI/CD

For automated testing, consider:

1. **Mock API calls** for unit tests
2. **Run integration tests** (real APIs) nightly
3. **Set timeout limits** to prevent hanging

Example CI configuration:
```yaml
# Run only fast tests in CI
pytest tests/test_researcher_deep.py::TestResearcherEdgeCases -v --timeout=60

# Run full suite nightly
pytest tests/test_researcher_deep.py -v --timeout=300
```

## Test Development

### Adding New Tests

1. Create test in appropriate class:
```python
@pytest.mark.asyncio
async def test_my_new_feature(self):
    researcher = Researcher()
    payload = {"text": "test query", "user_id": 12345}
    result = await researcher.execute(payload)
    assert result.success is True
```

2. Run your new test:
```bash
pytest tests/test_researcher_deep.py::TestClassName::test_my_new_feature -v -s
```

3. Verify it passes with real APIs

### Best Practices

1. âœ… Test with real APIs (at least once)
2. âœ… Verify logging output
3. âœ… Check edge cases
4. âœ… Measure performance
5. âœ… Document expected behavior

## Performance Baseline

Based on current implementation (2 workers, 3 search APIs):

| Metric | Target | Current |
|--------|--------|---------|
| Simple query | <15s | ~8-12s âœ… |
| Complex query | <45s | ~25-35s âœ… |
| Cache hit | <10s | ~3-8s âœ… |
| Success rate | >90% | ~95% âœ… |
| Output quality | >200 chars | ~400-1500 chars âœ… |

## Summary

This test suite provides:
- âœ… **Real-world validation** (actual API calls)
- âœ… **Edge case coverage** (empty, long, special chars)
- âœ… **Performance benchmarks** (timing, concurrency)
- âœ… **Quality assurance** (output validation)
- âœ… **Logging verification** (actual log messages)

**Run the tests** to ensure your researcher agent is working correctly before deploying to production!
